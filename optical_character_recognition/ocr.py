#!/usr/bin/python
#
# ./ocr.py : Perform optical character recognition, usage:
#     ./ocr.py train-image-file.png train-text.txt test-image-file.png
#
# Authors: (Akash Sheth)
# (based on skeleton code by D. Crandall, Oct 2017)
#
# Training data --> Used from the first part.
#
# HMM Abstraction:
# TRAIN_LETTERS = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789(),.-!?\"' "
# - States: Each of the possible 72 TRAIN LETTERS
# - Obsevables: Letters in the noisy images
# - Intial Probailities: Probability of sentences starting with each of the 72 characters
# - Transition Probability: Probability of transition from each each of the 72 characters to same 72 characters
# - Emission Probability: Probaility of images (each noisy letter) given all the possible 72 train letters
# - Program workflow:
#   - Initially, the model is being generated by calculating initial probabilities, transition probabilities and emission probabilities.
#   - After calculating above given probabilities we've used them to predict the each state corresponding to each noisy letter in the given test image using
#     algorithms(Simplified, Variable Elimination, Viterbi).
# - Model Assumptions:
#   - If the sentence does not start with particular character, default probability considered is 0.000000001
#   - If the a letter does not follow another letter then the default transtion probability considered is 0.000000001
#
#  ** Weights decided by trial and error
# Reference:
# [1] https://en.wikipedia.org/wiki/Precision_and_recall
# [2] https://stackoverflow.com/questions/26871866/print-highest-value-in-dict-with-key

from PIL import Image, ImageDraw, ImageFont
import sys
import math
import copy
CHARACTER_WIDTH = 14
CHARACTER_HEIGHT = 25
TRAIN_LETTERS = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789(),.-!?\"' "
total_pixel_per_box = CHARACTER_HEIGHT * CHARACTER_WIDTH
global i_prob  # check if required
global t_prob  # check if required
global e_prob  # check if required
global simplesequence  # check if required
global most_probable
global reverse_dict
global v_dict
global e_f_dict
global e_b_dict
global final_ve_dict
i_prob = {}
t_prob = {}
e_prob = {}
simplesequence = []
most_probable = []
reverse_dict = {}
v_dict = {}
e_f_dict = {}
e_b_dict = {}
final_ve_dict = {}


def init():
    for letter in TRAIN_LETTERS:
        letter = letter if letter != ' ' else "space"
        i_prob.update({letter: 1})

    for outer_letter in TRAIN_LETTERS:
        outer_letter = outer_letter if outer_letter != ' ' else "space"
        dict_x = {}
        for inner_letter in TRAIN_LETTERS:
            inner_letter = inner_letter if inner_letter != ' ' else "space"
            dict_x.update({inner_letter: 1})

        dict_x.update({"count": len(TRAIN_LETTERS)})
        t_prob.update({outer_letter: dict_x})
    # print len(t_prob)
    # print t_prob


def load_letters(fname):
    im = Image.open(fname)
    px = im.load()
    (x_size, y_size) = im.size
    result = []

    new_result = []
    for x_beg in range(0, int(x_size / CHARACTER_WIDTH) * CHARACTER_WIDTH, CHARACTER_WIDTH):
        result += [["".join(['*'if px[x, y] < 1 else '-' for x in range(x_beg, x_beg+CHARACTER_WIDTH)]) for y in range(0, CHARACTER_HEIGHT)], ]
    for index in range(0, len(result)):
        new_result.append(''.join(result[index]))
    return new_result


def print_letter(letter):

    a_list = []
    for x in range(0, len(letter)):
        if x % 14 == 0:
            print ''.join(a_list)
            print "",
            a_list = []
        a_list.append(letter[x])
    print ""


def load_training_letters(fname):
    letter_images = load_letters(fname)
    return_dict = {}
    for i in range(0, len(TRAIN_LETTERS)):
        character = TRAIN_LETTERS[i] if TRAIN_LETTERS[i] != ' ' else "space"
        return_dict.update({character: letter_images[i]})

    return return_dict


def read_data_and_generate_model(fname):
    global i_prob, t_prob
    file = open(fname, 'r')
    line_count = 0
    i_prob = {char if char != ' ' else "space": 0.000000001 for char in TRAIN_LETTERS}
    t_prob = {char if char != ' ' else "space": {char if char != ' ' else "space": 0.000000001 for char in TRAIN_LETTERS} for char in TRAIN_LETTERS}
    for line in file:
        line_count += 1
        if line[0] in TRAIN_LETTERS:
            i_prob[line[0]] += 1

        for letter_index in range(0, len(line)):
            if letter_index+1 == len(line):
                break

            if line[letter_index] not in TRAIN_LETTERS or line[letter_index+1] not in TRAIN_LETTERS:
                continue
            current_letter = line[letter_index] if line[letter_index] != ' ' else "space"
            next_letter = line[letter_index+1] if line[letter_index+1] != ' ' else "space"
            t_prob[current_letter][next_letter] += 1

    for key in i_prob.keys():
        i_prob[key] = float(i_prob[key]) / line_count

    for outer_letter in t_prob.keys():
        value_sum = sum(t_prob[outer_letter].values())
        for inner_letter in t_prob[outer_letter].keys():
            t_prob[outer_letter][inner_letter] = t_prob[outer_letter][inner_letter] / value_sum


#####
# main program
def calculate_emission(train, test):
    global e_prob
    train_pos_dict = {}
    for x, letter_image in train.items():
        x = x if x != ' ' else "space"
        pos_list = []
        for index in range(0, len(letter_image)):
            if letter_image[index] == "*":
                pos_list.append(index)
        train_pos_dict.update({x: pos_list})

    e_prob = {i: {char if char != ' ' else "space": 0 for char in TRAIN_LETTERS} for i in range(1, len(test)+1)}
    # Weighing pixels based on the their position
    # Weights decided by trial and error
    true_pos_weight = 0.05
    false_pos_weight = 0.02
    false_neg_weight = 0.98
    true_neg_weight = 0.95
    for test_index in range(0, len(test)):

        for letter, l_str in train.items():
            letter = letter if letter != ' ' else "space"

            pos_list = train_pos_dict[letter]
            # Reference [2] -- START
            # Weighing pixels based on the their position
            true_positive = 0
            true_negative = 0
            false_positive = 0
            false_negative = 0
            # Reference [2] -- END
            for index in range(0, CHARACTER_HEIGHT*CHARACTER_WIDTH):
                if test[test_index][index] == l_str[index]:
                    if index in pos_list:
                        # Means that it test pixel was supposed to be black and is black
                        true_positive += 1
                    else:
                        # Means that it test pixel was supposed to be white and is white
                        true_negative += 1
                else:
                    if index in pos_list:
                        # Means that it test pixel was supposed to be black and is white
                        false_negative += 1
                    else:
                        # Means that it test pixel was supposed to be white and is black
                        false_positive += 1
            if letter == 'space' or letter == '.':
                weight = (0.001**(float(false_positive))) * \
                         (1**(float(true_negative)))
            else:
                weight = (true_pos_weight**(float(true_positive))) * \
                         (false_neg_weight**(float(false_negative))) * \
                         (false_pos_weight**(float(false_positive))) * \
                         (true_neg_weight**(float(true_negative)))
            e_prob[test_index+1][letter] = weight

    for key in e_prob.keys():
        maximum = max(e_prob[key], key=e_prob[key].get)  # Reference [2]
        simplesequence.append(maximum if maximum != 'space' else ' ')
    print "Simple : ", ''.join(simplesequence)


def calculate_viterbi_RECURSION(v_letter_count):
    v_letter_count += 1
    if v_letter_count == 1:
        for key in train_letters.keys():
            key = key if key != ' ' else "space"
            i_e_prod = math.log(i_prob[key]) + math.log(e_prob[v_letter_count][key])

            if v_letter_count in v_dict.keys():
                v_dict[v_letter_count].update({key: i_e_prod})
            else:
                v_dict.update({v_letter_count: {key: i_e_prod}})

        return_letter = calculate_viterbi_RECURSION(v_letter_count)
        most_probable.append(return_letter if return_letter != "space" else ' ')

    else:
        max_of_j = ("", 0)
        return_if_not_last = ""
        for outer_key_j in train_letters.keys():
            outer_key_j = outer_key_j if outer_key_j != ' ' else "space"
            v_t_prod = 0
            max_of_v_t_prod = ("", -100000000)
            for inner_key_i in train_letters.keys():
                inner_key_i = inner_key_i if inner_key_i != ' ' else "space"
                if v_letter_count == 2:
                    v_t_prod = v_dict[v_letter_count-1][inner_key_i] + math.log(t_prob[inner_key_i][outer_key_j])
                else:
                    v_t_prod = v_dict[v_letter_count-1][inner_key_i][1] + math.log(t_prob[inner_key_i][outer_key_j])

                max_of_v_t_prod = (inner_key_i, v_t_prod) if v_t_prod > max_of_v_t_prod[1] else max_of_v_t_prod

            if v_letter_count in v_dict.keys():
                v_dict[v_letter_count].update({outer_key_j: [max_of_v_t_prod[0], max_of_v_t_prod[1] + math.log(e_prob[v_letter_count][outer_key_j])]})
            else:
                v_dict.update({v_letter_count: {outer_key_j: [max_of_v_t_prod[0], max_of_v_t_prod[1] + math.log(e_prob[v_letter_count][outer_key_j])]}})

        if v_letter_count == len(test_letters):
            for key, value in v_dict[v_letter_count].items():
                max_of_j = (key, value) if value > max_of_j[1] else max_of_j

            most_probable.append(max_of_j[0] if max_of_j[0] != "space" else ' ')
            return v_dict[v_letter_count][max_of_j[0]][0]
        else:
            return_if_not_last = calculate_viterbi_RECURSION(v_letter_count)
            most_probable.append(return_if_not_last if return_if_not_last != "space" else ' ')
            return v_dict[v_letter_count][return_if_not_last][0]

    most_probable.reverse()
    print "HMM MAP : ", ''.join([letter for letter in most_probable])


def variable_elemination():
    f_t_dict = {i: {char if char != ' ' else "space": 0 for char in TRAIN_LETTERS} for i in range(1, len(test_letters)+1)}
    e_f_dict = {i: {char if char != ' ' else "space": 0 for char in TRAIN_LETTERS} for i in range(1, len(test_letters)+1)}
    b_t_dict = {i: {char if char != ' ' else "space": 1 for char in TRAIN_LETTERS} for i in range(1, len(test_letters)+1)}
    e_b_dict = {i: {char if char != ' ' else "space": 0 for char in TRAIN_LETTERS} for i in range(1, len(test_letters)+1)}
    final_ve_dict = {i: {char if char != ' ' else "space": 0 for char in TRAIN_LETTERS} for i in range(1, len(test_letters)+1)}
    for f_count in range(1, len(test_letters)+1):
        # print f_count
        if f_count == 1:
            for letter in TRAIN_LETTERS:
                letter = letter if letter != ' ' else "space"
                f_t_dict[f_count][letter] = math.log(i_prob[letter])
                e_f_dict[f_count][letter] = f_t_dict[f_count][letter] + math.log(e_prob[f_count][letter])
        else:
            for outer_key in TRAIN_LETTERS:
                outer_key = outer_key if outer_key != ' ' else "space"
                for inner_key in TRAIN_LETTERS:
                    inner_key = inner_key if inner_key != ' ' else "space"
                    f_t_dict[f_count][outer_key] += f_t_dict[f_count-1][inner_key] + math.log(t_prob[inner_key][outer_key]) + math.log(e_prob[f_count-1][inner_key])
                f_t_dict[f_count][outer_key] *= 10**-2
                e_f_dict[f_count][outer_key] = f_t_dict[f_count][outer_key] + math.log(e_prob[f_count][outer_key])

    for b_count in range(len(test_letters), 0, -1):

        if b_count == len(test_letters):
            for letter in TRAIN_LETTERS:
                letter = letter if letter != ' ' else "space"
                e_b_dict[b_count][letter] = b_t_dict[b_count][letter] + math.log(e_prob[b_count][letter])
        else:
            for outer_key in TRAIN_LETTERS:
                outer_key = outer_key if outer_key != ' ' else "space"
                for inner_key in TRAIN_LETTERS:
                    inner_key = inner_key if inner_key != ' ' else "space"
                    b_t_dict[b_count][outer_key] += math.log(t_prob[outer_key][inner_key]) + math.log(e_prob[b_count+1][inner_key])
                b_t_dict[b_count][outer_key] *= 10**-2
                e_b_dict[b_count][outer_key] = b_t_dict[b_count][outer_key] + math.log(e_prob[b_count][outer_key])
    final_list = []
    for count in final_ve_dict.keys():
        max_final = ("", -100000000000)
        for letter in TRAIN_LETTERS:
            letter = letter if letter != ' ' else "space"
            final_ve_dict[count][letter] = e_f_dict[count][letter] + e_b_dict[count][letter]
            if final_ve_dict[count][letter] > max_final[1]:
                max_final = (letter, final_ve_dict[count][letter])
        final_list.append(max_final[0] if max_final[0] != "space" else ' ')

    print "HMM VE :", ''.join([x for x in final_list])


(train_img_fname, train_txt_fname, test_img_fname) = sys.argv[1:]
train_letters = load_training_letters(train_img_fname)

test_letters = load_letters(test_img_fname)

read_data_and_generate_model(train_txt_fname)
calculate_emission(train_letters, test_letters)

variable_elemination()
v_letter_count = 0
calculate_viterbi_RECURSION(v_letter_count)
